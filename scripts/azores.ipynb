{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8148ec62-2b7c-4c05-9b65-b38f1f5c1d35",
   "metadata": {},
   "source": [
    "# Outline of script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b35aa8-7384-4906-9d77-f463ff0cebc2",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e13d2c-e10b-4b5b-8f75-7a70dc1be3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07517718-7cad-49ae-a03a-2e8d9d3f0910",
   "metadata": {},
   "source": [
    "# Define constants/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a0834-084d-487d-b868-d1728c067148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "RAD_PER_DEG = 2 * np.pi / 360  # radians per degree\n",
    "R = 6.371e6  # radius of earth in m\n",
    "M_PER_KM = 1000  # meters per km\n",
    "\n",
    "## Filepaths\n",
    "# on clidex\n",
    "# era_fp = \"/vortexfs1/share/clidex/data/reanalysis/20CR/prmsl/prmsl.mon.mean.nc\"\n",
    "lme_fp = \"/vortex/clidex/data/model/CESM/LME/atm/psl\"\n",
    "\n",
    "# on cmip5 server (note: LME only has single ensemble member at the given directory)\n",
    "era_fp = \"/mnt/cmip5-data/reanalysis/era.20c/sfc/msl/moda/msl.mon.mean.nc\"\n",
    "noaa_fp = \"/mnt/cmip5-data/reanalysis/noaa.cires.20crv2c/monolevel/prmsl/monthly/prmsl.mon.mean.nc\"\n",
    "# lme_fp = \"/mnt/cmip5-data/CMIP5/output1/NCAR/CCSM4/past1000/mon/atmos/Amon/r1i1p1/psl/psl_Amon_CCSM4_past1000_r1i1p1_085001-185012.nc\"\n",
    "\n",
    "## Set plotting defaults\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d2cae-bd42-4433-9666-2c9a86b8141d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2aa9b-746b-4c3f-8b62-fc87265b5829",
   "metadata": {},
   "source": [
    "## Datasets used:\n",
    "- ERA-20C\n",
    "- NOAA–CIRES 20CR\n",
    "- HadSLP2\n",
    "- CESM1 LME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f345-4d61-4958-94c7-2c97d5c180ee",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dacba5-dcd4-4bbe-b9a4-cd51256e2a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trim_to_azores(data):\n",
    "    \"\"\"helper function to trim data to Azores lon/lat range.\"\"\"\n",
    "\n",
    "    ## Azores lon/lat range\n",
    "    lon_range = [-65, 15]\n",
    "    lat_range = [15, 65]\n",
    "\n",
    "    return data.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "\n",
    "\n",
    "def djf_avg(data):\n",
    "    \"\"\"function to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## subset data\n",
    "    data_seasonal_avg = data.resample(time=\"QS-DEC\").mean()\n",
    "\n",
    "    ## get annual average for DJF\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def djf_avg_alt(data):\n",
    "    \"\"\"alternative (and slightly more general) function\n",
    "    to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## get 3-month rolling average average for DJF\n",
    "    data_seasonal_avg = data_djf.resample({\"time\": \"3MS\"}, label=\"left\").mean()\n",
    "\n",
    "    ## subset for djf avg\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def convert_longitude(longitude):\n",
    "    \"\"\"move longitude from range [0,360) to (-180,180].\n",
    "    Function accepts and returns a numpy array representing longitude values\"\"\"\n",
    "\n",
    "    ## find indices of longitudes which will become negative\n",
    "    is_neg = longitude > 180\n",
    "\n",
    "    ## change values at these indices\n",
    "    longitude[is_neg] = longitude[is_neg] - 360\n",
    "\n",
    "    return longitude\n",
    "\n",
    "\n",
    "def update_longitude_coord(data):\n",
    "    \"\"\"move longitude for dataset or dataarray from [0,360)\n",
    "    to (-180, 180]. Function accepts xr.dataset/xr.dataarray\n",
    "    and returns object of same type\"\"\"\n",
    "\n",
    "    ## get updated longitude coordinate\n",
    "    updated_longitude = convert_longitude(data.longitude.values)\n",
    "\n",
    "    ## sort updated coordinate to be increasing\n",
    "    updated_longitude = np.sort(updated_longitude)\n",
    "\n",
    "    ## sort data (\"reindex\") according to update coordinate\n",
    "    data = data.reindex({\"longitude\": updated_longitude})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def standardize_lonlat(\n",
    "    data, rename_coords=False, reverse_latitude=False, update_longitude=False\n",
    "):\n",
    "    \"\"\"update lonlat coordinates to be consistent across datasets.\n",
    "    In particular, make sure:\n",
    "        - coordinates are called \"longitude\" and \"latitude\"\n",
    "        - latitude is increasing\n",
    "        - longitude is increasing and in the range (-180, 180].\n",
    "    Function takes in and returns xr.dataarray or xr.dataset\"\"\"\n",
    "\n",
    "    ## change coord names from \"lat\" and \"lon\" to\n",
    "    ## \"latitude\" and \"longitude\", respectively\n",
    "    if rename_coords:\n",
    "        data = data.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "    ## change longitude from range [0,360) to (-180, 180]\n",
    "    if update_longitude:\n",
    "        data = update_longitude_coord(data)\n",
    "\n",
    "    ## switch direction of latitude so that it's increasing\n",
    "    if reverse_latitude:\n",
    "        latitude_updated = data.latitude.values[::-1]\n",
    "        data = data.reindex({\"latitude\": latitude_updated})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_era():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(era_fp)[\"msl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, update_longitude=True, reverse_latitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_noaa():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(noaa_fp)[\"prmsl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(\n",
    "        data, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme_member(member_id):\n",
    "    \"\"\"\n",
    "    Function loads data from single member of CESM last-millenium ensemble (LME).\n",
    "    Args:\n",
    "    - 'member_id' is integer in [1,13] specifying which ensemble member to load\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get names of two files for each ensemble member\n",
    "    prefix = f\"{lme_fp}/b.e11.BLMTRC5CN.f19_g16.{member_id:03d}.cam.h0.PSL\"\n",
    "    fname0 = f\"{prefix}.085001-184912.nc\"\n",
    "    fname1 = f\"{prefix}.185001-200512.nc\"\n",
    "\n",
    "    ## Load data\n",
    "    data = xr.open_mfdataset([fname0, fname1], chunks={\"time\": 2000})[\"PSL\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, rename_coords=True, update_longitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme(member_ids=np.arange(1, 14).astype(int)):\n",
    "    \"\"\"load multiple LME members. 'member_ids' is an\n",
    "    array-like object specifying which ensemble members to load. This\n",
    "    is a list of integers in the range [1,13]\"\"\"\n",
    "\n",
    "    ## Load all ensemble members in list\n",
    "    data = [load_lme_member(i) for i in tqdm(member_ids)]\n",
    "\n",
    "    ## convert list to xarray\n",
    "    data = xr.concat(data, dim=pd.Index(member_ids, name=\"ensemble_member\"))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def spatial_avg(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"get global average of a quantity over the sphere.\n",
    "    Data is xr.dataarray/xr.dataset with  a 'regular' lon/lat grid\n",
    "    (equal lon/lat spacing between all data points)\"\"\"\n",
    "\n",
    "    ## get latitude-dependent weights\n",
    "    # first, convert latitude from degrees to radians\n",
    "    # conversion factor = (2 pi radians)/(360 degrees)\n",
    "    latitude_radians = data.latitude * (2 * np.pi) / 360\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## Next, trim data to specified range\n",
    "    data_trim = data.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "    cos_lat_trim = cos_lat.sel(latitude=slice(*lat_range))\n",
    "\n",
    "    ## Next, compute weighted avg\n",
    "    data_weighted = data_trim.weighted(weights=cos_lat_trim)\n",
    "    data_avg = data_weighted.mean([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_avg\n",
    "\n",
    "\n",
    "def spatial_int(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"compute spatial integral of a quantity on the sphere. For convenience,\n",
    "    assume regular grid (constant lat/lon)\"\"\"\n",
    "\n",
    "    ## Get latitude/longitude in radians.\n",
    "    ## denote (lon,lat) in radians as (theta, phi)\n",
    "    rad_per_deg = 2 * np.pi / 360\n",
    "    theta = data.longitude * RAD_PER_DEG\n",
    "    phi = data.latitude * RAD_PER_DEG\n",
    "\n",
    "    ## get differences for integration (assumes constant differences)\n",
    "    dtheta = theta[1] - theta[0]\n",
    "    dphi = phi[1] - phi[0]\n",
    "\n",
    "    ## broadcast to grid\n",
    "    dtheta = dtheta * xr.ones_like(theta)\n",
    "    dphi = dphi * xr.ones_like(phi)\n",
    "\n",
    "    ## Get area of patch\n",
    "    dA = R**2 * np.cos(phi) * dphi * dtheta\n",
    "\n",
    "    ## Integrate\n",
    "    data_int = (data * dA).sum([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_int\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"year\"):\n",
    "    \"\"\"Get linear trend for an xr.dataarray along specified dimension\"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = data.polyfit(dim=dim, deg=1)[\"polyfit_coefficients\"]\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0d3ae-fdeb-461c-92af-066469610b69",
   "metadata": {},
   "source": [
    "## Do the actual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc2a14-188c-4cac-96df-e1fcfea9a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Load\" data (but not into memory yet)\n",
    "# slp_lme = load_lme(member_ids=[1, 2])\n",
    "slp_lme = load_lme(member_ids=np.arange(1, 14))\n",
    "slp_noaa = load_noaa()\n",
    "slp_era = load_era()\n",
    "\n",
    "## Get DJF average\n",
    "slp_noaa = djf_avg(slp_noaa).compute()\n",
    "slp_era = djf_avg(slp_era).compute()\n",
    "slp_lme = djf_avg(slp_lme).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26f044-848b-484b-a782-39003f27bb93",
   "metadata": {},
   "source": [
    "# Compute AHA metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f1cc-1712-4d7b-a4fd-3da50b3f4cb0",
   "metadata": {},
   "source": [
    "From Cresswell-Clay et al. (2022): \"The AHA was defined as the area (km2) over the North Atlantic and Western Europe that had mean winter (December–January–February) SLP greater than 0.5 s.d. from the mean of the spatio-temporal winter SLP distribution (Fig. 1b). The region considered when calculating the AHA is bounded by the 60° W and 10° E meridians as well as the 10° N and 52° N latitudes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a71ec-5841-4414-99b4-36716affe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AHA(slp, norm_type=\"global_mean\"):\n",
    "    \"\"\"compute Azores High Area index, similar to Cresswell-Clay et al. (2022).\n",
    "    Defined as: area of Azores region which has (normalized) SLP exceeding 0.5 std\n",
    "    of long term average. Normalized SLP is defined as local SLP minus\n",
    "    globally-averaged SLP.\n",
    "\n",
    "    Args:\n",
    "    - slp is gridded SLP data (at global scale)\n",
    "    - norm_type is one of {\"global_mean\", \"detrend\"} specifying\n",
    "        how to normalize the AHA index\n",
    "\n",
    "    Note: returns area in KM^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get SLP anomaly in Azores regions\n",
    "    slp_azores = trim_to_azores(slp)\n",
    "\n",
    "    ## get globally-averaged SLP\n",
    "    slp_global_avg = spatial_avg(slp)\n",
    "\n",
    "    ## get normalized anomaly (func of lon/lat)\n",
    "    if norm_type == \"global_mean\":\n",
    "        slp_azores_norm = slp_azores - slp_global_avg\n",
    "\n",
    "    elif norm_type == \"detrend\":\n",
    "        trend = get_trend(slp_global_avg)\n",
    "        slp_azores_norm = slp_azores - trend\n",
    "\n",
    "    else:\n",
    "        print(\"Error: specify valid normalization type.\")\n",
    "\n",
    "    ## Get standard deviation (func of lon/lat)\n",
    "    slp_azores_mean = slp_azores_norm.mean(\"year\")\n",
    "    slp_azores_std = slp_azores_norm.std(\"year\")\n",
    "\n",
    "    ## Get mask of grid cells exceeding 0.5 std threshold\n",
    "    threshold = slp_azores_mean + 0.5 * slp_azores_std\n",
    "    exceeds_thresh = slp_azores_norm > threshold\n",
    "\n",
    "    ## Sum area of lon/lat cells exceeding threshold\n",
    "    ## convert from True/False to 1.0/0.0 for integration\n",
    "    AHA = spatial_int(exceeds_thresh.astype(float))\n",
    "\n",
    "    ## convert from m^2 to km^2\n",
    "    m2_per_km2 = 1e6\n",
    "    AHA *= 1 / (M_PER_KM**2)\n",
    "\n",
    "    return AHA\n",
    "\n",
    "\n",
    "def count_extremes(AHA, cutoff_perc=90.0, window=25):\n",
    "    \"\"\"Get rolling count of Azores High extreme events.\n",
    "    Args:\n",
    "    - cutoff_perc is percentile value in range (0 and 100) used to define\n",
    "        'extreme' events\n",
    "    - window is an integer specifying how many years the rolling window is.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get threshold for extreme events\n",
    "    threshold = AHA.quantile(q=cutoff_perc / 100)\n",
    "\n",
    "    ## Get boolean array: True if AHA exceeds thresh\n",
    "    exceeds_thresh = AHA > threshold\n",
    "\n",
    "    ## Get rolling count\n",
    "    rolling_count = exceeds_thresh.rolling(dim={\"year\": window}, center=True).sum()\n",
    "\n",
    "    ## remove NaN values at beginning and end\n",
    "    nan_count = np.round((window - 1) / 2).astype(int)\n",
    "    rolling_count = rolling_count.isel(year=slice(nan_count, -nan_count))\n",
    "\n",
    "    return rolling_count\n",
    "\n",
    "\n",
    "def count_extremes_wrapper(slp, norm_type=\"detrend\"):\n",
    "    \"\"\"wrapper function which takes in SLP and computes # of Azores High extremes\"\"\"\n",
    "    return count_extremes(compute_AHA(slp, norm_type=norm_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7e10-e422-461d-843a-5de9b744a1da",
   "metadata": {},
   "source": [
    "Look at SLP over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8048-7657-48f0-94e5-ef4ed9972c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute area-averaged SLP\n",
    "slp_azores = spatial_avg(trim_to_azores(slp_noaa))\n",
    "slp_global = spatial_avg(slp_noaa)\n",
    "\n",
    "## get linear trend for global\n",
    "trend_fit = get_trend(slp_global)\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_azores, label=\"Azores\")\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_global, label=\"Global\")\n",
    "ax.plot(trend_fit.year, 1e-2 * trend_fit, label=\"Global trend\", c=\"k\", ls=\"--\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()\n",
    "\n",
    "## Plot before/after normalizing\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(\n",
    "    slp_noaa.year,\n",
    "    1e-2 * (slp_azores - slp_global + slp_global.mean()),\n",
    "    label=\"remove global mean\",\n",
    ")\n",
    "ax.plot(\n",
    "    trend_fit.year,\n",
    "    1e-2 * (slp_azores - trend_fit + trend_fit.mean()),\n",
    "    label=\"remove trend\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32599842-f855-44a2-a44f-77a9d805a58f",
   "metadata": {},
   "source": [
    "Plot # of LME extremes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112e69-e42b-4480-a6d7-6137107a3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify which type of normalization\n",
    "## one of {\"global_mean\",\"detrend\"}\n",
    "norm_type = \"global_mean\"\n",
    "\n",
    "## count extremes in reanalysis\n",
    "count_noaa = count_extremes_wrapper(slp_noaa, norm_type=norm_type)\n",
    "count_era = count_extremes_wrapper(slp_era, norm_type=norm_type)\n",
    "\n",
    "## count in historical component of LME\n",
    "slp_lme_hist = slp_lme.sel(year=slice(1850, None))\n",
    "count_lme = count_extremes_wrapper(slp_lme_hist, norm_type=norm_type)\n",
    "count_lme_mean = count_lme.mean(\"ensemble_member\")\n",
    "count_lme_min = count_lme.min(\"ensemble_member\")\n",
    "count_lme_max = count_lme.max(\"ensemble_member\")\n",
    "\n",
    "## make plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot reanalysis\n",
    "ax.plot(count_noaa.year, count_noaa, label=\"NOAA\", ls=\"--\")\n",
    "ax.plot(count_era.year, count_era, label=\"ERA\", ls=\":\")\n",
    "\n",
    "## plot LME mean and range\n",
    "count_lme_plot = ax.plot(count_lme.year, count_lme_mean, label=\"LME\", ls=\"-\")\n",
    "for bound in [count_lme_min, count_lme_max]:\n",
    "    ax.plot(bound.year, bound, c=count_lme_plot[0].get_color(), lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count (25-yr rolling)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e15d4f-6f95-4cf6-8134-4fa3db9d87b9",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06f1a7-be80-4bd4-a15c-a0f832362f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim to Azores region\n",
    "# slp_noaa_azores =\n",
    "\n",
    "# ## now subset in space and get DJF average (loads into memory)\n",
    "# print(\"Loading LME\")\n",
    "# slp_lme = reduce(slp_lme)\n",
    "\n",
    "# print(\"Loading NOAA\")\n",
    "# slp_noaa = reduce(slp_noaa)\n",
    "\n",
    "# print(\"Loading 20CR\")\n",
    "# slp_era = reduce(slp_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f12b-3dff-4c19-94e7-fea0312a5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cos_lat = np.cos(slp_noaa.latitude * 2 * np.pi / 360)\n",
    "slp_noaa_weighted = slp_noaa.weighted(weights=cos_lat)\n",
    "slp_weighted = slp_noaa_weighted.mean([\"longitude\", \"latitude\"])\n",
    "slp_unweighted = slp_noaa.mean([\"longitude\", \"latitude\"])\n",
    "# (slp_noaa * cos_lat).sum(\"latitude\") / cos_lat.sum(\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15222b7e-d4e5-431e-84a3-e07a4e2e44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_azores = trim_to_azores(slp_noaa)\n",
    "slp_azores = slp_azores.weighted(np.cos(slp_azores.latitude * 2 * np.pi / 360))\n",
    "slp_azores = slp_azores.mean([\"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cccac-3922-404e-a708-049bb8a87052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "y0 = 1900\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_weighted.sel(year=slice(y0, None))\n",
    ")\n",
    "\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_unweighted.sel(year=slice(y0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c990-9a5e-4f24-89bb-7e43a133668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_weighted.year, slp_weighted, label=\"weighted\")\n",
    "ax.plot(slp_weighted.year, spatial_avg(slp_noaa), label=\"weighted\", ls=\"--\", c=\"k\")\n",
    "ax.plot(slp_unweighted.year, slp_unweighted, label=\"unweighted\")\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.plot(\n",
    "    slp_azores.year, slp_azores - slp_weighted + slp_weighted.mean(), label=\"azores\"\n",
    ")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b14fad-a724-4a75-83c2-95b0f2e8ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.stats.pearsonr(slp_azores / slp_weighted, slp_azores - slp_weighted))\n",
    "print(\n",
    "    scipy.stats.pearsonr(\n",
    "        np.log(slp_azores) - np.log(slp_weighted), slp_azores - slp_weighted\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e19d7-cca6-4ed5-b463-a694d862687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda x: (x - x.mean()) / x.std()\n",
    "plt.plot(norm(slp_azores / slp_weighted))\n",
    "plt.plot(norm(slp_azores - slp_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a330e9-cced-4091-8a1e-1d2d72f4cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anom(data):\n",
    "    \"\"\"Compute anomalies relative to long-term mean\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def AHA(data_anom):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a64e26-ddba-45e0-be67-5648ff84c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_noaa.chunk({\"time\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6603f3-d94b-4507-8061-93207a451a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = slp_noaa.isel(latitude=10, longitude=10)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(djf_avg(x))\n",
    "ax.plot(djf_avg2(x))\n",
    "ax.set_xlim([-1, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ad98a-b0ef-4671-9e03-27d49ac788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115dcd-c38e-4664-b752-10e12687210e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef31b92-0635-42d1-94a7-53df7ac3beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2857f-467f-4b08-b1ad-35900ef91e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg(slp_noaa).year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff0ac0-89b1-4a5a-afc3-36b2feff3aaa",
   "metadata": {},
   "source": [
    "# Compute AHA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d778e6c-87b4-483a-80e4-2ade90e2fa87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
