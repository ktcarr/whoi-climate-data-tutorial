{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e0832c-bcae-43d2-bf3e-cddc35962480",
   "metadata": {},
   "source": [
    "# Overview:  loading and prepping data from WHOI's \"CMIP\" server\n",
    "\n",
    "The goal of this tutorial is to demonstrate – and provide a reference for – how to load and pre-process gridded climate data from WHOI's servers. We'll start by [opening](#Opening-data) and [plotting](#Plotting-data) some temperature data and perform some common pre-processing steps, including [spatial averaging](#Spatial-averaging), [removing the seasonal cycle](#Seasonal-cycle), [interpolation and resampling](#Interpolation-and-resampling), [detrending](#Detrending), [computing correlations](#Correlation), and [regridding](#Regridding-data). These examples are not necessarily the \"best\" (e.g., most efficient) way to do things and are intended as a starting point (the 2-dimenstional dataset we're working with is small enough that it doesn't matter too much if we do things inefficiently).\n",
    "\n",
    "Whenever possible, we've attempted to use built-in functions from [xarray](https://xarray.dev/), a Python package useful for working with gridded climate data and netcdf files. I think of it as a high-level \"wrapper\" for lower-level packages like ```numpy``` and ```pandas```. While the core of a ```xarray.DataArray``` object is a ```numpy.array```, the ```xarray``` object includes dimension names and additional metadata. This tends to make code easier to interpret: for example, to average over latitudes in a ```numpy``` array, you have to keep track of which array dimension corresponds to latitude – e.g., ```data.mean(axis=2)```, if latitude is the 2$^{nd}$ dimension – whereas in ```xarray``` you don't: ```data.mean(dim=\"latitude\")```.  \n",
    "\n",
    "For a more complete tutorial on the ```xarray``` package, I highly recommend the developers' official [45-minute tutorial](https://tutorial.xarray.dev/overview/xarray-in-45-min.html).  \n",
    "\n",
    "[__Your task:__ Define your own climate index](#To-do:-define-your-own-climate-index) and compute it using a reanalysis product of your choice (we'll assess climate change using this index in the next part of the tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db42837-02ad-43fe-9390-3fb3c0c2b653",
   "metadata": {},
   "source": [
    "## Check if we're running in Google Colab\n",
    "If you are running in Google Colab, you may have to run the cell below twice because the kernel crashes; I'm not sure why this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be59d6-4bac-4d7c-b6eb-befd667c9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ## install package that allows us to use mamba in Colab\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "\n",
    "    condacolab.install()\n",
    "\n",
    "    ## install extra packages to colab environment\n",
    "    !mamba install -c conda-forge python=3.10.13 cmocean xesmf cartopy cftime cartopy\n",
    "\n",
    "    ## connect to Google Drive (will prompt you to ask for permissions)\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    ## flag telling us the notebook is running in Colab\n",
    "    IN_COLAB = True\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a02ae-e820-4024-bd67-4e13fbe3cf11",
   "metadata": {},
   "source": [
    "## Filepaths\n",
    "__To run this notebook, you'll need to update the filepaths below__, which specify the location of the data (otherwise, you'll get a ```FileNotFoundError``` message when you try to open the data). These filepaths will differ for Mac vs. Windows users and depend on how you've accessed the data (e.g., mounting the WHOI file server or downloading the data).\n",
    "\n",
    "Below, we've defined two sets of filepaths: one for the cloud (to be used with Google Colab) and another for data on CMIP server/local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea616a-943f-42a0-b3de-25fab19ae023",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "\n",
    "    ## path to ERA5 reanalysis 2m-temperature data\n",
    "    era5_t2m_path = \"/content/drive/My Drive/climate-data/era5/2m_temperature\"\n",
    "\n",
    "    ## path to ERA5 reanalysis sea level pressure data\n",
    "    era5_slp_path = \"/content/drive/My Drive/climate-data/era5/mean_sea_level_pressure\"\n",
    "\n",
    "    ## path to MIROC6 SST data\n",
    "    miroc6_path = \"/content/drive/My Drive/climate-data\"\n",
    "\n",
    "else:\n",
    "\n",
    "    ## path to ERA5 reanalysis 2m-temperature data\n",
    "    era5_t2m_path = (\n",
    "        \"/Volumes/cmip6/data/era5/reanalysis/single-levels/monthly-means/2m_temperature\"\n",
    "    )\n",
    "\n",
    "    ## path to ERA5 reanalysis sea level pressure data\n",
    "    era5_slp_path = \"/Volumes/cmip6/data/era5/reanalysis/single-levels/monthly-means/mean_sea_level_pressure\"\n",
    "\n",
    "    ## path to MIROC6 sea surface temperature data\n",
    "    miroc6_path = (\n",
    "        \"/Volumes/cmip6/data/cmip6/CMIP/MIROC/MIROC6/historical/r1i1p1f1/Omon/tos/gn/1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c416fa-d265-4595-83a3-95bd470ad5fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319a253-bef7-456e-b46b-f824a0c74014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "from matplotlib.dates import DateFormatter\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import cftime\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import os.path\n",
    "\n",
    "## set plotting style\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})\n",
    "\n",
    "## initialize random number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f248b-4640-4e33-9e1a-0cb594181ec8",
   "metadata": {},
   "source": [
    "## Opening data\n",
    "We're going to work with a \"reanalysis\" product located on the CMIP6 archive. As a reminder from lecture, a reanalysis is a hybrid of model output and observations. Observations (e.g., of rainfall, temperature, or ocean salinity) are sparse (& irregular) in time in space. The purpose of the reanalysis is to fill in the gaps, creating a nice \"gridded\" dataset which *is* regular in time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbcc89-1931-481d-af8f-99b21db8d24f",
   "metadata": {},
   "source": [
    "Note: for the ERA5 reanalysis, each year of data has a separate file. We'll print out the names of the first 4 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3194f3-f9f7-4853-92a2-394338678bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List the first few files in the 2m-temperature folder\n",
    "file_pattern = os.path.join(era5_t2m_path, \"*.nc\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "print(np.sort(file_list)[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426736fb-cf5f-45dc-befb-06c84e599f71",
   "metadata": {},
   "source": [
    "To open the dataset, use ```xr.open_dataset``` (single file) or ```xr.open_mfdataset``` (multiple files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596beb39-4a23-4683-af1b-45ab6959dccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load a single file using xarray\n",
    "T2m_1980 = xr.open_dataset(os.path.join(era5_t2m_path, \"1980_2m_temperature.nc\"))\n",
    "T2m_1980.load()\n",
    "# loads into memory\n",
    "\n",
    "## open the first 3 files (but don't load to memory)\n",
    "T2m = xr.open_mfdataset(file_list[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f74c47-df73-4ae5-bf6c-0e643e09758f",
   "metadata": {},
   "source": [
    "To subset data, use the ```.isel``` / ```.sel``` functions. We'll write a function which trims data to the North Atlantic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34adb9d-aadf-43d3-92b8-682a131db5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select data for Jan 1., two different ways\n",
    "print(\n",
    "    np.allclose(\n",
    "        T2m_1980[\"t2m\"].isel(time=0).values,\n",
    "        T2m_1980[\"t2m\"].sel(time=\"1980-01-01\").values,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def trim_to_north_atl(x):\n",
    "    \"\"\"trims data to the North Atlantic region\"\"\"\n",
    "\n",
    "    ## lon/lat boundaries for N. Atlantic\n",
    "    lon_range = [260, 360]\n",
    "    lat_range = [70, 3]\n",
    "\n",
    "    ## trim the data\n",
    "    x_trimmed = x.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "\n",
    "    return x_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c7349-c247-44c3-a49b-711994d337dc",
   "metadata": {},
   "source": [
    "If we only care about a subset of the data (i.e., not the whole globe), it can be helpful to subset it *while* loading. To do this, pass a subsetting function to ```xr.open_mfdataset```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3728702-95d4-47ef-a0d2-e1b519428e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load trimmed data\n",
    "T2m_trimmed = xr.open_mfdataset(file_list[:3], preprocess=trim_to_north_atl).compute()\n",
    "\n",
    "## Compare size to original data\n",
    "print(f\"Shape of raw data:     {T2m['t2m'].shape}\")\n",
    "print(f\"Shape of trimmed data: {T2m_trimmed['t2m'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7d37c-05a8-4f90-b1c3-5d3f7acb9e37",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eee878-fb80-441a-b0e6-2187447d0cf4",
   "metadata": {},
   "source": [
    "First, let's define a function which draws a blank map (don't worry about the details for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae3b0e-1dfe-4b8f-8b9d-6621eb1ae29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, a generic plot setup function\n",
    "def plot_setup(ax, lon_range, lat_range, xticks, yticks, scale):\n",
    "    \"\"\"\n",
    "    Create map background for plotting spatial data.\n",
    "    Arguments:\n",
    "        - ax: Matplotlib object containing everything in the plot.\n",
    "            (I think of it as the plot \"canvas\")\n",
    "        - lon_range/lat_range: 2-element arrays, representing plot boundaries\n",
    "        - xticks/yticks: location for lon/lat labels\n",
    "        - scale: number which controls linewidth and fontsize\n",
    "\n",
    "    Returns a modified 'ax' object.\n",
    "    \"\"\"\n",
    "\n",
    "    ## specify transparency/linewidths\n",
    "    grid_alpha = 0.1 * scale\n",
    "    grid_linewidth = 0.5 * scale\n",
    "    coastline_linewidth = 0.3 * scale\n",
    "    label_size = 8 * scale\n",
    "\n",
    "    ## crop map and plot coastlines\n",
    "    ax.set_extent([*lon_range, *lat_range], crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(linewidth=coastline_linewidth)\n",
    "\n",
    "    ## plot grid\n",
    "    gl = ax.gridlines(\n",
    "        draw_labels=True,\n",
    "        linestyle=\"--\",\n",
    "        alpha=grid_alpha,\n",
    "        linewidth=grid_linewidth,\n",
    "        color=\"k\",\n",
    "        zorder=1.05,\n",
    "    )\n",
    "\n",
    "    ## add tick labels\n",
    "    gl.bottom_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {\"size\": label_size}\n",
    "    gl.ylabel_style = {\"size\": label_size}\n",
    "    gl.ylocator = mticker.FixedLocator(yticks)\n",
    "    gl.xlocator = mticker.FixedLocator(xticks)\n",
    "\n",
    "    return ax, gl\n",
    "\n",
    "\n",
    "## Next, a function to plot the North Atlantic\n",
    "def plot_setup_north_atl(ax, scale=1):\n",
    "    \"\"\"Create map background for plotting spatial data.\n",
    "    Returns modified 'ax' object.\"\"\"\n",
    "\n",
    "    ## specify range and ticklabels for plot\n",
    "    lon_range = [-100, 0]\n",
    "    lat_range = [3, 70]\n",
    "    xticks = [-80, -60, -40, -20, 0]\n",
    "    yticks = [20, 40, 60]\n",
    "\n",
    "    ax, gl = plot_setup(ax, lon_range, lat_range, xticks, yticks, scale)\n",
    "\n",
    "    return ax, gl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35469d-c92b-46ab-bc02-4f7a8ad9bc4b",
   "metadata": {},
   "source": [
    "Next, plot a sample of the temperature dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7f373-6f9d-4f23-ae87-88175e5d02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a figure object (can contain multiple \"Axes\" object)\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "## add Axes object (blank canvas for our plot)\n",
    "ax = fig.add_subplot(projection=ccrs.PlateCarree())\n",
    "ax, gl = plot_setup_north_atl(ax)\n",
    "\n",
    "## Let's plot T2m data for Jan 1, 1980\n",
    "t2m_plot = ax.contourf(\n",
    "    T2m_1980.longitude,\n",
    "    T2m_1980.latitude,\n",
    "    T2m_1980[\"t2m\"].isel(time=0),\n",
    "    levels=np.arange(260, 304, 4),  # contour levels to plot\n",
    "    cmap=\"cmo.thermal\",  # colormap (see https://matplotlib.org/cmocean/)\n",
    "    extend=\"both\",  # includes values outside of contour bounds\n",
    ")\n",
    "\n",
    "## add a colorbar\n",
    "cb = fig.colorbar(t2m_plot, orientation=\"vertical\", label=r\"$K$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2974b60-3371-4b09-bfc8-470ecbc200bf",
   "metadata": {},
   "source": [
    "## Spatial averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0969f0e-25d2-49b5-8b0e-c28440285b34",
   "metadata": {},
   "source": [
    "Next, write a function to spatially average the data. This is slightly more involved than just averaging over all elements in the array: we need to compute a weighted average, with the weights given by the cosine of the latitude . The reason is that we're averaging on the surface of a sphere, and need to account for the fact that grid cells get smaller as you go closer to the poles (see [below](#Explanation-of-cos(latitude)-weighting) for a more detailed explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9d528-0940-419d-9b58-a1890d6fa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_avg(data):\n",
    "    \"\"\"function to compute spatial average of data on grid with constant\n",
    "    longitude/latitude spacing.\"\"\"\n",
    "\n",
    "    ## first, compute cosine of latitude (after converting degrees to radians)\n",
    "    latitude_radians = np.deg2rad(data.latitude)\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## get weighted average using xarray\n",
    "    avg = data.weighted(weights=cos_lat).mean([\"longitude\", \"latitude\"])\n",
    "\n",
    "    return avg\n",
    "\n",
    "\n",
    "## \"naive\" unweighted average over longitude and latitudes\n",
    "avg_unweighted = T2m_trimmed[\"t2m\"].mean([\"latitude\", \"longitude\"])\n",
    "\n",
    "## (correct) weighted average\n",
    "avg_weighted = spatial_avg(T2m_trimmed[\"t2m\"])\n",
    "\n",
    "## compare results:\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(avg_unweighted.time, avg_unweighted, label=\"unweighted\")\n",
    "ax.plot(avg_weighted.time, avg_weighted, label=\"weighted\")\n",
    "\n",
    "## label plot\n",
    "ax.legend(prop={\"size\": 10})\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(r\"$K$\")\n",
    "ax.set_xticks(ax.get_xticks()[::3])\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcb783-7fd2-4f97-b7a8-e5395c2b081a",
   "metadata": {},
   "source": [
    "### Explanation of cos(latitude) weighting\n",
    "Not every element in our array represents an equal surface area of the earth: on a \"regular\" longitude-latitude grid (where the longitude and latitude spacing is constant) the area of gridcells decreases as you move away from the equator to the poles. Denoting longitude and latitude as $\\theta$ and $\\phi$, and longitude and latitude spacing as $\\delta\\theta$ and $\\delta\\phi$, the area of each gridcell is given by $\\delta A ~= R^2\\cos(\\phi)\\delta\\phi\\delta\\theta$, where $R$ is the radius of the earth. The weighted average of a variable $f$ on the sphere is then given by:\n",
    "\\begin{align}\n",
    "    \\overline{f} &= \\frac{\\sum f~\\delta A}{\\sum \\delta A} = \\frac{\\sum f~R^2\\cos\\left(\\phi\\right)~\\delta\\phi~\\delta\\theta}{\\sum R^2\\cos\\left(\\phi\\right)~\\delta\\phi~\\delta\\theta} = \\frac{R^2 ~\\delta\\phi~\\delta\\theta \\sum f~\\cos\\left(\\phi\\right)}{R^2~\\delta\\phi~\\delta\\theta \\sum \\cos(\\phi)} = \\frac{\\sum f\\cos(\\phi)}{\\sum\\cos(\\phi)},\n",
    "\\end{align}\n",
    "where we use the fact that $R^2$, $\\delta\\theta$, and $\\delta\\phi$ are constant (and can be pulled out of the summation). Therefore, to compute $\\overline{f}$, we need to compute a *weighted* average, where the weights are the cosine of the latitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0a3b3-48d1-4668-9d75-77c9298f69bd",
   "metadata": {},
   "source": [
    "## Seasonal cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438eafd-efe6-42f1-be22-97370cacda08",
   "metadata": {},
   "source": [
    "Use ```groupby``` to group data by month (and apply functions to each month separately). Below, we'll compute the mean temperature for each season and subtract it from the data to obtain anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f6e1f-95be-47f6-a025-49573f103a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_cyc = avg_weighted.groupby(\"time.month\").mean()\n",
    "anomalies = avg_weighted.groupby(\"time.month\") - seasonal_cyc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42586515-a795-4bef-a462-e847d2357754",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71289dc3-df44-46f2-9af2-32832e3491c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot seasonal cycle\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "ax.plot(seasonal_cyc.month, seasonal_cyc)\n",
    "ax.set_xticks([1, 6, 11], labels=[\"Jan\", \"Jun\", \"Nov\"])\n",
    "ax.set_yticks(ticks=[285, 290, 295])\n",
    "ax.set_ylabel(r\"$K$\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_title(\"Mean seasonal cycle\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Plot anomalies\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "\n",
    "## plot raw data\n",
    "p = ax.plot(avg_weighted.time, avg_weighted, label=\"w/ seasonal cycle\", alpha=0.5)\n",
    "ax.set_ylabel(r\"$T$ before ($K$)\", color=p[0].get_color())\n",
    "ax.set_yticks(ticks=[285, 290, 295], labels=[285, 290, 295], color=p[0].get_color())\n",
    "\n",
    "## plot after removing mean seasonal cycle\n",
    "ax_anom = ax.twinx()\n",
    "ax_anom.plot(anomalies.time, anomalies, label=\"w/o seasonal cycle\", c=\"k\")\n",
    "ax_anom.set_ylabel(r\"$T$ after ($K$)\")\n",
    "\n",
    "## Label plot\n",
    "ax.set_xticks(ax.get_xticks()[::3])\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.set_title(\"Before/after removing seasonal cycle\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbd802-9b68-4468-a9bf-267f3c060435",
   "metadata": {},
   "source": [
    "Note we can compute the seasonal cycle at every gridpoint separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1aff0-73d7-4696-ad45-1180793f3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute seasonal cycle, and plot at diff. b/n july and jan\n",
    "seasonal_cyc_spatial = T2m_trimmed[\"t2m\"].groupby(\"time.month\").mean()\n",
    "jul_jan_diff = seasonal_cyc_spatial.sel(month=7) - seasonal_cyc_spatial.sel(month=1)\n",
    "\n",
    "## Create a figure object (can contain multiple \"Axes\" object)\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(projection=ccrs.PlateCarree())\n",
    "ax, gl = plot_setup_north_atl(ax)\n",
    "\n",
    "## Plot data\n",
    "t2m_plot = ax.contourf(\n",
    "    jul_jan_diff.longitude,\n",
    "    jul_jan_diff.latitude,\n",
    "    jul_jan_diff,\n",
    "    levels=np.arange(-30, 34, 4),\n",
    "    cmap=\"cmo.balance\",\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "## add a colorbar and label\n",
    "cb = fig.colorbar(\n",
    "    t2m_plot, orientation=\"vertical\", label=r\"$K$\", ticks=np.arange(-30, 45, 15)\n",
    ")\n",
    "ax.set_title(\"Jul – Jan difference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd54c8e-fb41-40de-90ef-e1e5e1e9e321",
   "metadata": {},
   "source": [
    "Check that the order of averaging doesn't matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74669be7-cbcc-4439-8ff9-dfba093e4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  to spatial dataset\n",
    "print(np.allclose(spatial_avg(seasonal_cyc_spatial), seasonal_cyc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bca5be-925f-4e9e-869a-fa1527cc0f83",
   "metadata": {},
   "source": [
    "## Interpolation and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f372ea3-cc52-4623-8f48-7a0ab1add942",
   "metadata": {},
   "source": [
    "### Averaging in time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4556c-c837-4d3e-abb3-97d94b8a759d",
   "metadata": {},
   "source": [
    "Below, let's downsample from monthly data to 3-month averages, grouped into the following months: Dec-Jan-Feb (DJF), Mar-Apr-May (MAM), Jun-Jul-Aug (JJA), and Sep-Oct-Nov (SON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575d1f8-a292-4b97-be3d-7a8769cefdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resample to quarterly (3-month) averages, where\n",
    "## the first quarter starts in December (\"QS-DEC\"),\n",
    "## and each point is labeled by the first month in the quarter\n",
    "anomalies_seasonal = anomalies.resample({\"time\": \"QS-DEC\"}).mean()\n",
    "\n",
    "## To extract the Dec-Jan-Feb season:\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot data\n",
    "ax.step(anomalies_seasonal.time, anomalies_seasonal, where=\"post\", label=\"seasonal\")\n",
    "ax.plot(anomalies.time, anomalies, c=\"k\", alpha=0.3, label=\"monthly\")\n",
    "\n",
    "## label plot\n",
    "ax.set_ylabel(r\"$T$ anomaly ($K$)\")\n",
    "ax.set_yticks([-0.5, 0.0, 0.5])\n",
    "ax.set_xticks(ax.get_xticks()[::3])\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9348649-3e4a-4432-a2de-80a40de96270",
   "metadata": {},
   "source": [
    "Next, downsample to annual data two different ways. Note that the resulting time axis differs for each method: one is labeled with a ```datetime``` object representing the first day in the year, while the other is labeled by an integer representing the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51c957-8334-4bcb-8e4b-0d62f696a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## annual mean, 2 different ways\n",
    "anomalies_annual0 = anomalies.resample({\"time\": \"YS-JAN\"}).mean()\n",
    "anomalies_annual1 = anomalies.groupby(\"time.year\").mean()\n",
    "print(np.allclose(anomalies_annual0, anomalies_annual1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b313eb-341e-4b17-a19c-929064522698",
   "metadata": {},
   "source": [
    "### Resampling in space\n",
    "```xarray```'s ```interp``` function can be used to resample in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa822db2-a3c4-4bdc-907d-15b8e4695c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsample by averaging in space\n",
    "lat_coarse = T2m_trimmed.latitude[::4]\n",
    "lon_coarse = T2m_trimmed.longitude[::4]\n",
    "\n",
    "## Do the downsampling (linear interpolation is default)\n",
    "T2m_coarse = T2m_trimmed.interp({\"longitude\": lon_coarse, \"latitude\": lat_coarse})\n",
    "\n",
    "print(f\"Original shape:     {T2m_trimmed['t2m'].shape}\")\n",
    "print(f\"After downsampling: {T2m_coarse['t2m'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cda6e1-f5ae-4a59-9250-04191d568e22",
   "metadata": {},
   "source": [
    "## Detrending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bfa15-f805-4369-b9c2-3e77ffb88604",
   "metadata": {},
   "source": [
    "To illustrate detrending, let's load in a longer timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b2159-2026-4b7b-8450-97c9e1325425",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a preprocessing function\n",
    "def preprocess(data):\n",
    "    \"\"\"pre-process data before loading it:\n",
    "    1. Downsample in space\n",
    "    2. only get DJF months\n",
    "    \"\"\"\n",
    "    ## 1. Downsample in space\n",
    "    data_ = data.interp({\"longitude\": lon_coarse, \"latitude\": lat_coarse}).compute()\n",
    "\n",
    "    ## 2. Find indices of winter season\n",
    "    month = data_.time.dt.month\n",
    "    is_winter = (month == 12) | (month <= 2)\n",
    "\n",
    "    ## select winter season\n",
    "    data_ = data_.sel(time=is_winter)\n",
    "\n",
    "    return data_\n",
    "\n",
    "\n",
    "def djf_avg(data):\n",
    "    \"\"\"function to get Dec-Jan-Feb average\"\"\"\n",
    "\n",
    "    ## first, resample from monthly to seasonal\n",
    "    data_ = data.resample({\"time\": \"QS-DEC\"}).mean()\n",
    "\n",
    "    ## next, select DJF season\n",
    "    is_winter = data_.time.dt.month == 12\n",
    "    data_ = data_.sel(time=is_winter)\n",
    "\n",
    "    ## for convenience, replace \"time\" index with \"year\":\n",
    "    ## label with year for Jan, (so Dec-'78,Jan-'79,Feb-'79 -> 1979)\n",
    "    year = data_.time.dt.year.values + 1\n",
    "    data_[\"time\"] = year\n",
    "    data_ = data_.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_\n",
    "\n",
    "\n",
    "## load prepped data\n",
    "T2m_long = xr.open_mfdataset(file_list, preprocess=preprocess)\n",
    "\n",
    "## Get DJF average\n",
    "T2m_djf = djf_avg(T2m_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf39e6-c4af-4acb-9dc0-c6becf67e433",
   "metadata": {},
   "source": [
    "Next, write a function to compute a trend along a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7103e9c-c2d4-48c4-a033-d2735edfe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend(data, dim=\"year\"):\n",
    "    \"\"\"Get linear trend for an xr.dataarray along specified dimension\"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = data.polyfit(dim=dim, deg=1)[\"polyfit_coefficients\"]\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f8344-78ad-4d84-9cba-2f6a6d5a96d4",
   "metadata": {},
   "source": [
    "Compute the trend and remove it from data to obtain a \"detrended\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654e2fc-2b5f-4291-ba6a-2964484de349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the trend\n",
    "T2m_trend = get_trend(T2m_djf[\"t2m\"])\n",
    "\n",
    "## Get detrended version\n",
    "T2m_detrended = T2m_djf[\"t2m\"] - T2m_trend\n",
    "\n",
    "## compute spatial averages\n",
    "T2m_trend_avg = spatial_avg(T2m_trend)\n",
    "T2m_detrended_avg = spatial_avg(T2m_detrended)\n",
    "T2m_djf_avg = spatial_avg(T2m_djf[\"t2m\"])\n",
    "\n",
    "\n",
    "## plot result\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "ax.plot(T2m_djf_avg.year, T2m_djf_avg, label=\"DJF\")\n",
    "ax.plot(T2m_trend_avg.year, T2m_trend_avg, label=\"DJF trend\", c=\"k\", alpha=0.3)\n",
    "ax.plot(\n",
    "    T2m_detrended_avg.year,\n",
    "    T2m_detrended_avg + T2m_trend_avg.mean(),\n",
    "    label=\"DJF detrended\",\n",
    ")\n",
    "ax.set_yticks([285, 286])\n",
    "ax.set_xticks([1980, 2000, 2020])\n",
    "ax.legend(prop={\"size\": 8})\n",
    "ax.set_title(r\"North Atlantic $T_{2m}$\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(r\"$T_{2m}$ ($K$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8672a-8e23-4ac6-9b00-3c9eaae0b862",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a9afb-d99d-46d2-9b7c-ca48bf3368a9",
   "metadata": {},
   "source": [
    "Load in sea level pressure from ERA5 and detrend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05d665-16fd-4302-a685-bd42f3823f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get list of SLP files\n",
    "slp_file_pattern = os.path.join(era5_slp_path, \"*.nc\")\n",
    "slp_file_list = glob.glob(slp_file_pattern)\n",
    "\n",
    "## Open SLP over N. Atlantic\n",
    "slp = xr.open_mfdataset(slp_file_list, preprocess=preprocess)[\"msl\"]\n",
    "\n",
    "## convert from Pa to hPa (1 hPa = 100 Pa)\n",
    "slp *= 1 / 100\n",
    "\n",
    "## Get DJF average\n",
    "slp_djf = djf_avg(slp)\n",
    "\n",
    "## detrend\n",
    "slp_detrended = slp_djf - get_trend(slp_djf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2e68d-9f05-4c8e-a31c-e609b0e88743",
   "metadata": {},
   "source": [
    "### Compute point-wise correlation between SLP and $T_{2m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3067b-e982-4637-9404-c98f0ca14fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cb_range(amp, delta):\n",
    "    \"\"\"Make colorbar_range for cmo.balance\n",
    "    Args:\n",
    "        - 'amp': amplitude of maximum value for colorbar\n",
    "        - 'delta': increment for colorbar\n",
    "    \"\"\"\n",
    "    return np.concatenate(\n",
    "        [np.arange(-amp, 0, delta), np.arange(delta, amp + delta, delta)]\n",
    "    )\n",
    "\n",
    "\n",
    "def make_correlation_plot(correlation):\n",
    "\n",
    "    ## Plot results\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    ax = fig.add_subplot(projection=ccrs.PlateCarree())\n",
    "    ax, gl = plot_setup_north_atl(ax)\n",
    "\n",
    "    ## Plot data\n",
    "    corr_plot = ax.contourf(\n",
    "        correlation.longitude,\n",
    "        correlation.latitude,\n",
    "        correlation,\n",
    "        levels=make_cb_range(1, 0.1),\n",
    "        cmap=\"cmo.balance\",\n",
    "        extend=\"both\",\n",
    "    )\n",
    "\n",
    "    ## add a colorbar and label\n",
    "    cb = fig.colorbar(corr_plot, orientation=\"vertical\", ticks=[-1, 0, 1])\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "## compute correlation\n",
    "slp_T2m_corr = xr.corr(slp_detrended, T2m_detrended, dim=\"year\")\n",
    "\n",
    "## plot results\n",
    "fig, ax = make_correlation_plot(slp_T2m_corr)\n",
    "ax.set_title(r\"Correlation b/n $T_{2m}$ and SLP during winter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002a22e-029d-4bfe-bbae-92f2c969771a",
   "metadata": {},
   "source": [
    "### Correlation between a single point the rest of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32449107-df25-4c8b-ba6f-a4217bef0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get SLP near Woods Hole\n",
    "T2m_woodshole = T2m_detrended.interp(latitude=41.5, longitude=288.5)\n",
    "\n",
    "## get correlation between Woods Hole T2m and North Atlantic SLP\n",
    "T2mWH_slp_corr = xr.corr(T2m_woodshole, slp_detrended, dim=\"year\")\n",
    "\n",
    "fig, ax = make_correlation_plot(T2mWH_slp_corr)\n",
    "ax.scatter(288.5, 41.5, marker=\"*\", c=\"k\", s=100)\n",
    "ax.set_title(r\"Correlation b/n Woods Hole SLP and $T_{2m}$ during winter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3929d-2f31-40a9-a0da-68849dc680ac",
   "metadata": {},
   "source": [
    "## Regridding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d60caf-b46a-4afd-9254-209398f7b9e4",
   "metadata": {},
   "source": [
    "Next, we're going to load data on a non-regular grid (tripolar, in this case). Specifically, we'll look at sea surface temperature (SST) from the MIROC6 model's historical simulation. We'll show how to plot the data on it's native grid and how to regrid it to a regular lon/lat grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d667-3d4b-44e5-9541-7f24910a0d8e",
   "metadata": {},
   "source": [
    "Start by importing ```xesmf``` package for regridding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314820f-258a-40f6-8432-4ec04c3cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8797d82-01a9-4a0c-a526-e3e9001892d2",
   "metadata": {},
   "source": [
    "First, load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a763ee-5ce6-4276-83fc-a3905098cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"tos_Omon_MIROC6_historical_r1i1p1f1_gn_195001-201412.nc\"\n",
    "miroc6_fullpath = os.path.join(miroc6_path, fname)\n",
    "sst_miroc6 = xr.open_dataset(miroc6_fullpath)\n",
    "sst_miroc6 = sst_miroc6.isel(time=slice(-36, None)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cc258-1157-4d56-ae17-840fb0519b27",
   "metadata": {},
   "source": [
    "Next, regrid to a \"regular\" lon/lat grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767c06b-4d2b-42a7-b049-29b5aba7bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## should we use custom grid?\n",
    "use_custom_grid = True\n",
    "\n",
    "if use_custom_grid:\n",
    "    # create regular lon/lat grid\n",
    "    grid = xr.DataArray(\n",
    "        data=None,\n",
    "        coords={\"longitude\": np.arange(0, 360), \"latitude\": np.arange(-90, 91)},\n",
    "        dims=[\"longitude\", \"latitude\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # use ERA5 grid\n",
    "    grid = T2m_detrended.isel(year=0)\n",
    "\n",
    "## do the regridding\n",
    "regridder = xe.Regridder(ds_in=sst_miroc6, ds_out=grid, method=\"bilinear\")\n",
    "sst_miroc6_regridded = regridder(sst_miroc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeee0ad-bdc6-4f33-a290-ce0272ee8dcc",
   "metadata": {},
   "source": [
    "Try using CDO if ESMF doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388a74b-2c32-48a2-8c3b-c5f8acf91f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify \"target\" grid\n",
    "target_grid = T2m_detrended.isel(year=0, drop=True)\n",
    "target_grid.to_netcdf(\"target_grid.nc\")\n",
    "\n",
    "## Get text file description of the grid (needed for regridding)\n",
    "!cdo griddes target_grid.nc > target_grid.txt\n",
    "\n",
    "## Do the regridding with CDO\n",
    "!cdo -O remapbil,target_grid.txt {miroc6_fullpath} sst_miroc6_regridded.nc\n",
    "\n",
    "## remove temporary files\n",
    "!rm target_grid.txt target_grid.nc\n",
    "\n",
    "## Load in regridded data\n",
    "sst_miroc6_regridded = xr.open_dataset(\"sst_miroc6_regridded.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f47992-aee5-453e-8f2f-d226debdf2fa",
   "metadata": {},
   "source": [
    "Plot comparison of data on native and regular grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e58094-1c06-461b-af7b-fad1b972f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the plot\n",
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "\n",
    "## canvas for plot on native grid\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax, gl = plot_setup_north_atl(ax)\n",
    "\n",
    "## canvas for plot with regridded data\n",
    "ax_regridded = fig.add_subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "ax_regridded, gl_regridded = plot_setup_north_atl(ax_regridded)\n",
    "\n",
    "## Plot data on native grid\n",
    "ax.set_title(\"Native grid\")\n",
    "sst_plot = ax.pcolormesh(\n",
    "    sst_miroc6.longitude,\n",
    "    sst_miroc6.latitude,\n",
    "    sst_miroc6[\"tos\"].mean(\"time\"),\n",
    "    cmap=\"cmo.thermal\",\n",
    "    vmin=-2,\n",
    "    vmax=32,\n",
    ")\n",
    "cb = fig.colorbar(sst_plot)\n",
    "\n",
    "## Plot regridded data\n",
    "ax_regridded.set_title(\"Regridded\")\n",
    "xx, yy = np.meshgrid(sst_miroc6_regridded.longitude, sst_miroc6_regridded.latitude)\n",
    "sst_plot_regridded = ax_regridded.pcolormesh(\n",
    "    xx,\n",
    "    yy,\n",
    "    sst_miroc6_regridded[\"tos\"].mean(\"time\"),\n",
    "    cmap=\"cmo.thermal\",\n",
    "    vmin=-2,\n",
    "    vmax=32,\n",
    ")\n",
    "cb = fig.colorbar(sst_plot_regridded, ticks=[0, 10, 20, 30])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c2ac9-0dda-41f8-832d-e40410c4cbf5",
   "metadata": {},
   "source": [
    "# To-do: define your own climate index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77828e6-9b80-43f2-b411-1bb79ae997c4",
   "metadata": {},
   "source": [
    "The first task in the tutorial is to define your own climate index. By \"climate index\", we mean any metric that can be compared between current and future simulations of climate. This can be as simple as surface temperature in a given location (as in the Woods Hole temperature example above) – in fact, this is what we recommend for the tutorial. In particular, we suggest picking any location on the globe and defining as an index the 2-meter air temperature or sea surface temperature closest to that point. One could also experiment with area-averaging over a given region (e.g., a 5$^{\\circ}\\times 5^{\\circ}$ box)$^\\dagger$. If you don't have a preferred reanalysis product, we recommend using ERA5.\n",
    "\n",
    "Some possible questions to consider include:\n",
    "- should the index be an annual or seasonal (or monthly) average?\n",
    "- how sensitive is the index to the size of the \"box\" used for area-averaging?\n",
    "\n",
    "$^\\dagger$For an extra challenge, one could use a more-involved index: e.g., globally-averaged sea surface temperature, mean latitude of the jet stream, or Arctic sea ice extent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
